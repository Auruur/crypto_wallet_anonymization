# ğŸ” Breaking and Rebuilding Privacy: Microdata Anonymization and Differential Privacy on Cryptocurrency Wallets Data

**Authors:** Fernando Gonzalez Domenech & Giovanni Murgia

---

## ğŸ“˜ Project Overview

This repository contains two complementary privacy studies applied to the same synthetic **cryptocurrency wallet dataset**, focusing on:

* **Part 1:** Microdata anonymization (k-anonymity, suppression, generalization)
* **Part 2:** Differential Privacy (Local and Central mechanisms)

The objective is to understand how different privacy frameworks protect sensitive financial data while preserving analytical utility.

---

# ğŸ§¾ Dataset

* **Rows:** 10,000
* **Attributes:** 10
* **Source:** Generated via Mockaroo with custom Ruby scripts
* The dataset contains:

  * **Identifiers:** e.g., `wallet_address`
  * **Quasi-identifiers:** `country`, `network`, `client_type`, etc.
  * **Sensitive attributes:** `balance_usd`, `activity_score`, `sanctions_score`, `high_risk`

---

# ğŸ”’ Part 1 â€” Microdata Protection Techniques

This part applies classical anonymization techniques to achieve **k â‰¥ 5 anonymity** while maintaining utility for downstream analysis.

## ğŸŸ¦ Identifier Masking

Sensitive direct identifiers (`wallet_address`) were masked by keeping the first 6 characters and replacing the rest with `*`.

## ğŸŸ© Quasi-Identifier Generalization

Performed generalization on attributes that could indirectly identify users:

* Countries outside the top 10 â†’ grouped into â€œOtherâ€
* Client types grouped (human vs. non-human)
* Numeric attributes binned into intervals (e.g., balance brackets)

## ğŸŸ¥ Local Suppression

Quasi-identifier combinations appearing in fewer than **k = 5** records were suppressed or removed to enforce k-anonymity.

## ğŸŸ¨ Optional Noise Perturbation

Small random noise added to numeric attributes (e.g., `balance_usd`) to preserve analytical trends.

## ğŸ“Š Evaluation: k-Anonymity + Utility

* Verified that all QI combinations satisfy **k â‰¥ 5**
* Trained a Logistic Regression model before/after anonymization
* Observed minimal performance degradation â†’ **utility preserved**

---

# ğŸ” Part 2 â€” Differential Privacy Protection

The second part focuses on **Local** and **Central** Differential Privacy to protect sensitive financial risk indicators while still supporting statistical analysis.

---

## ğŸŸ¦ Local Differential Privacy (LDP)

LDP protects users **before data collection**, meaning the server never sees their true values.

### 1. **Trivial Coin Toss Mechanism**

* Returns the true `high_risk` value with 50% probability
* Otherwise returns a random coin flip
* Introduced as a simple baseline for data perturbation (non-parameterized)

### 2. **Îµ-Coin Toss / Randomized Response**

Each user perturbs their own binary value using:

[
p = \frac{e^\varepsilon}{e^\varepsilon + 1}
]

* With probability (p), return the true value
* With probability (1 - p), return a fair coin flip
* Using the inverse estimator, we reconstructed the true `high_risk` rate

### 3. **Microsoft 1-Bit LDP Mechanism**

For numeric values like `sanctions_score`:

* Users normalize (X_i) to ([0,1])
* Each user sends **one single DP-protected bit**
* The server aggregates bits and uses an unbiased estimator to recover the original mean
* Demonstrates strong LDP with minimal communication

---

## ğŸŸ¥ Central Differential Privacy (CDP)

In CDP, a trusted curator holds raw data and adds noise to **query outputs**.

### 1. **Counting Query (High-Risk Count)**

* Sensitivity = 1
* Laplace noise added to the true count
* Running the mechanism for different Îµ values shows how noisy counts converge toward the true count as Îµ increases

### 2. **Mean Query (Balance USD)**

* Values clipped to ([L, U]) (1stâ€“99th percentile) to bound sensitivity
  â†’ prevents â€œwhaleâ€ outliers dominating the mean
* Sensitivity = ((U - L) / n)
* Laplace noise added to the mean
* KDE plots show noisy means approaching the true clipped mean for larger Îµ

---

## ğŸŸ§ Above Threshold (Sparse Vector Technique)

A CDP mechanism that:

* Tests many queries while spending only **one Îµ**
* Adds noise to both the threshold and each query
* Returns **only the first query** whose noisy value exceeds the noisy threshold

In our dataset:

* Queries = mean `sanctions_score` for the top countries
* Threshold = 75th percentile of country means
* Output revealed **only the first passing country** (e.g., *Slovakia*)
* All true means, noisy values, and comparisons remained hidden

---

# ğŸ“Š Key Findings

* Differential Privacy enables useful analysis of sensitive financial data without exposing individual information.
* Experiments with multiple Îµ values highlight the **privacyâ€“utility tradeoff**:
  Stronger privacy â†’ more noise; weaker privacy â†’ higher accuracy.
* Choosing the right model (Local vs Central DP) is essential:
  LDP when no trust in the collector, CDP when accurate statistics are needed.

---

# ğŸ“ Files in This Repository

| File                                | Description                                                                              |
| ----------------------------------- | ---------------------------------------------------------------------------------------- |
| `PPIA_HW1.ipynb`                    | Microdata anonymization techniques (masking, suppression, generalization).               |
| `PPIA_HW2.ipynb` | Differential Privacy mechanisms (LDP, CDP, Laplace, RR, Microsoft LDP, Above Threshold). |

---

# ğŸ§° Requirements

Install dependencies:

```bash
pip install pandas numpy scikit-learn matplotlib seaborn
```

---
